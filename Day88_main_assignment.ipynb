{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Day88 Main Assignment**\n",
    "# Selecting the best model with Best hyperparameters\n",
    "\n",
    "**Author:** Shahid Umar\\\n",
    "**Enrolled:** In Data Science and AI Course\\\n",
    "**Email:** shahidcontacts@gmail.com\\\n",
    "**Contact:** +923455516634\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- ### <span style=\"color:pink\">Code to convert the time into minutes, and seconds is stored in the variable 'total_time'</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Start time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow;\">**1)  IMPORT LIBRARIES AND LOAD THE DATASET**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Import the necessary libraries</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.98 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Above code is majid command to measure the time it takes to run this code\n",
    "\n",
    "# Import Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import train test split the data libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import regression algorithms libraris\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Import GridSearchCV library for cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import preprocessors libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# To remove warnings from output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # this is to display all the columns in the dataframe\n",
    "pd.set_option('display.max_rows', None) # this is to display all the rows in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Load the dataset for regression tasks</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 348 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load dataset\n",
    "df = sns.load_dataset('tips')\n",
    "# This dataset is loaded for performing regression tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow;\">**2) DATA PREPROCESSING FOR REGRESSION**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display top 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   total_bill  244 non-null    float64 \n",
      " 1   tip         244 non-null    float64 \n",
      " 2   sex         244 non-null    category\n",
      " 3   smoker      244 non-null    category\n",
      " 4   day         244 non-null    category\n",
      " 5   time        244 non-null    category\n",
      " 6   size        244 non-null    int64   \n",
      "dtypes: category(4), float64(2), int64(1)\n",
      "memory usage: 7.4 KB\n"
     ]
    }
   ],
   "source": [
    "# To check the dataset brief information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are four categorical variables  in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_bill    0\n",
       "tip           0\n",
       "sex           0\n",
       "smoker        0\n",
       "day           0\n",
       "time          0\n",
       "size          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check null or missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow;\">**3) REGRESSION TASKS**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Lable encoding the categorical variables (Independent variables)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# select features and variables\n",
    "X = df.drop('tip', axis=1) # Independent variables\n",
    "y = df['tip'] # Dependent variable\n",
    "\n",
    "# label encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X['sex'] = le.fit_transform(X['sex'])\n",
    "X['smoker'] = le.fit_transform(X['smoker'])\n",
    "X['day'] = le.fit_transform(X['day'])\n",
    "X['time'] = le.fit_transform(X['time'])\n",
    "\n",
    "# fit_transform: This method fits a transformation model to the data and applies the transformation to the dataset, returning the transformed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow\">**3.1) HYPERPARAMETER TUNING FOR REGRESSION MODELS**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Split the data into train and test data with 80% training dataset and predict the best model with evaluation of `regression metrics`</span>\n",
    "- ### <span style=\"color:pink\">To Choose the best model through *`for loop`*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression MSE: 0.9010765093466211\n",
      "LinearRegression R2: 0.2688449578261525\n",
      "LinearRegression MAE: 0.7030677287148895\n",
      "\n",
      "SVR MSE: 0.722510769711588\n",
      "SVR R2: 0.4137374719904898\n",
      "SVR MAE: 0.6353762826521168\n",
      "\n",
      "DecisionTreeRegressor MSE: 1.1103059656945642\n",
      "DecisionTreeRegressor R2: 0.09907117014743683\n",
      "DecisionTreeRegressor MAE: 0.8256388209661746\n",
      "\n",
      "RandomForestRegressor MSE: 0.798906332244899\n",
      "RandomForestRegressor R2: 0.3517482844281148\n",
      "RandomForestRegressor MAE: 0.7049877551020411\n",
      "\n",
      "KNeighborsRegressor MSE: 0.7213128467265923\n",
      "KNeighborsRegressor R2: 0.4147094953664522\n",
      "KNeighborsRegressor MAE: 0.647888198757764\n",
      "\n",
      "GradientBoostingRegressor MSE: 0.6662961023916425\n",
      "GradientBoostingRegressor R2: 0.4593513982539841\n",
      "GradientBoostingRegressor MAE: 0.6740958714953833\n",
      "\n",
      "XGBRegressor MSE: 0.7565200490013941\n",
      "XGBRegressor R2: 0.3861415289429111\n",
      "XGBRegressor MAE: 0.6937729422900142\n",
      "\n",
      "Best Regression Model: GradientBoostingRegressor(n_estimators=10)\n",
      "Best MSE: 0.6662961023916425\n",
      "Best R2: 0.4593513982539841\n",
      "Best MAE: 0.6740958714953833\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test data with 80% training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a dictionary of models with hyperparameters to evaluate\n",
    "models = { \n",
    "          'LinearRegression' : (LinearRegression(), {}),\n",
    "          'SVR' : (SVR(), {'kernel': ['rbf', 'poly', 'sigmoid']}),\n",
    "          'DecisionTreeRegressor' : (DecisionTreeRegressor(), {'max_depth': [None, 5, 10]}),\n",
    "          'RandomForestRegressor' : (RandomForestRegressor(), {'n_estimators': [10, 100]}),\n",
    "          'KNeighborsRegressor' : (KNeighborsRegressor(), {'n_neighbors': np.arange(3, 100, 2)}),\n",
    "          'GradientBoostingRegressor' : (GradientBoostingRegressor(), {'n_estimators': [10, 100]}),\n",
    "          'XGBRegressor' : (XGBRegressor(), {'n_estimators': [10, 100]}),          \n",
    "          }\n",
    "\n",
    "# Initialize variables to track the best model and its performance\n",
    "best_regression_model = None\n",
    "best_mse = float('inf')\n",
    "best_r2 = -float('inf')\n",
    "best_mae = float('inf')\n",
    "# float('inf') is used here because the code is initializing best_mse to a value that is guaranteed to be larger than any other real number.\n",
    "\n",
    "# Iterate over each model, train, predict, and evaluate performance metrics\n",
    "for name, (model, params) in models.items():\n",
    "    # Create a pipeline with the model\n",
    "    pipeline = GridSearchCV(model, params, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(name, 'MSE:', mse)\n",
    "    print(name, 'R2:', r2)\n",
    "    print(name, 'MAE:', mae)\n",
    "    print()\n",
    "    \n",
    "    # Check if this model has better performance\n",
    "    if mse < best_mse:\n",
    "        best_regression_model = pipeline\n",
    "        best_mse = mse\n",
    "        best_r2 = r2\n",
    "        best_mae = mae\n",
    "\n",
    "# Print the best model's performance metrics\n",
    "print('Best Regression Model:', best_regression_model.best_estimator_)\n",
    "print('Best MSE:', best_mse)\n",
    "print('Best R2:', best_r2)\n",
    "print('Best MAE:', best_mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow;\">**4) CLASSIFICATION TASKS**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Import the necessary libraries for classification and load the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Split the data into train and test data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test data with 80% training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Generate Dicturionary for Classfier models</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of classifiers to evaluate\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'XGBoost' : XGBClassifier(),\n",
    "    #'LightGBM': lgb.LGBMClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">Perform Cross-Validation with respect to mean accuracy</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Mean Accuracy: 0.9466666666666667\n",
      "\n",
      "Classifier: SVM\n",
      "Mean Accuracy: 0.9666666666666668\n",
      "\n",
      "Classifier: Random Forest\n",
      "Mean Accuracy: 0.9600000000000002\n",
      "\n",
      "Classifier: GradientBoosting\n",
      "Mean Accuracy: 0.9533333333333334\n",
      "\n",
      "Classifier: AdaBoost\n",
      "Mean Accuracy: 0.9466666666666667\n",
      "\n",
      "Classifier: XGBoost\n",
      "Mean Accuracy: 0.9400000000000001\n",
      "\n",
      "Classifier: KNN\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "\n",
      "Classifier: NaiveBayes\n",
      "Mean Accuracy: 0.9600000000000002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform k-fold cross-validation and calculate the mean accuracy\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    scores = cross_val_score(classifier, X, y, cv=kfold)\n",
    "    accuracy = np.mean(scores)\n",
    "    print(\"Classifier:\", name)\n",
    "    print(\"Mean Accuracy:\", accuracy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">**Method-1** -Select the best Classification model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Calssification Model:  KNN\n"
     ]
    }
   ],
   "source": [
    "best_classification_model_1 = {}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    scores = cross_val_score(classifier, X_train, y_train, cv=kfold)\n",
    "    best_classification_model_1[name] = scores.mean()\n",
    "\n",
    "best_classifier = [classifier for classifier, score in best_classification_model_1.items() if score == max(best_classification_model_1.values())][0]\n",
    "print(\"Best Calssification Model: \", best_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### <span style=\"color:pink\">**Method-2** -Select the best Classification model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 1.0\n",
      "Logistic Regression Precision: 1.0\n",
      "Logistic Regression Recall: 1.0\n",
      "Logistic Regression F1: 1.0\n",
      "\n",
      "Decision Tree Accuracy: 1.0\n",
      "Decision Tree Precision: 1.0\n",
      "Decision Tree Recall: 1.0\n",
      "Decision Tree F1: 1.0\n",
      "\n",
      "SVM Accuracy: 1.0\n",
      "SVM Precision: 1.0\n",
      "SVM Recall: 1.0\n",
      "SVM F1: 1.0\n",
      "\n",
      "Random Forest Accuracy: 1.0\n",
      "Random Forest Precision: 1.0\n",
      "Random Forest Recall: 1.0\n",
      "Random Forest F1: 1.0\n",
      "\n",
      "KNN Accuracy: 1.0\n",
      "KNN Precision: 1.0\n",
      "KNN Recall: 1.0\n",
      "KNN F1: 1.0\n",
      "\n",
      "GradientBoosting Accuracy: 1.0\n",
      "GradientBoosting Precision: 1.0\n",
      "GradientBoosting Recall: 1.0\n",
      "GradientBoosting F1: 1.0\n",
      "\n",
      "XGBoost Accuracy: 1.0\n",
      "XGBoost Precision: 1.0\n",
      "XGBoost Recall: 1.0\n",
      "XGBoost F1: 1.0\n",
      "\n",
      "AdaBoost Accuracy: 1.0\n",
      "AdaBoost Precision: 1.0\n",
      "AdaBoost Recall: 1.0\n",
      "AdaBoost F1: 1.0\n",
      "\n",
      "Naive Bayes Accuracy: 1.0\n",
      "Naive Bayes Precision: 1.0\n",
      "Naive Bayes Recall: 1.0\n",
      "Naive Bayes F1: 1.0\n",
      "\n",
      "Best Classification Model: LogisticRegression()\n",
      "Best Accuracy: 1.0\n",
      "Best Precision: 1.0\n",
      "Best Recall: 1.0\n",
      "Best F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Split the data into train and test data with 80% training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a dictionary of models with hyperparameters to evaluate\n",
    "models = { \n",
    "    'Logistic Regression': (LogisticRegression(), {}),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None, 5, 10]}),\n",
    "    'SVM': (SVC(), {'kernel': ['linear', 'rbf', 'poly']}),\n",
    "    'Random Forest': (RandomForestClassifier(), {'n_estimators': [10, 100]}),\n",
    "    'KNN': (KNeighborsClassifier(), {'n_neighbors': np.arange(3, 100, 2)}),\n",
    "    'GradientBoosting': (GradientBoostingClassifier(), {'n_estimators': [10, 100]}),\n",
    "    'XGBoost': (XGBClassifier(), {'n_estimators': [10, 100]}),\n",
    "    #'LightGBM': (lgb.LGBMClassifier(), {}),\n",
    "    'AdaBoost': (AdaBoostClassifier(), {}),\n",
    "    'Naive Bayes': (GaussianNB(), {}),\n",
    "}\n",
    "\n",
    "# Initialize variables to track the best model and its performance\n",
    "best_classification_model_2 = None\n",
    "best_accuracy = -float('inf')\n",
    "best_precision = -float('inf')\n",
    "best_recall = -float('inf')\n",
    "best_f1 = -float('inf')\n",
    "\n",
    "# Iterate over each model, train, predict, and evaluate performance metrics\n",
    "for name, (model, params) in models.items():\n",
    "    # Create a pipeline with the model\n",
    "    pipeline = GridSearchCV(model, params, cv=5) # 5-fold cross-validation\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Print the performance metrics\n",
    "    print(name, 'Accuracy:', accuracy)\n",
    "    print(name, 'Precision:', precision)\n",
    "    print(name, 'Recall:', recall)\n",
    "    print(name, 'F1:', f1)\n",
    "    print()\n",
    "    \n",
    "    # Check if this model has better performance\n",
    "    if accuracy > best_accuracy:\n",
    "        best_classification_model_2 = pipeline\n",
    "        best_accuracy = accuracy\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "        best_f1 = f1\n",
    "\n",
    "# Print the best model's performance metrics\n",
    "print('Best Classification Model:', best_classification_model_2.best_estimator_)\n",
    "print('Best Accuracy:', best_accuracy)\n",
    "print('Best Precision:', best_precision)\n",
    "print('Best Recall:', best_recall)\n",
    "print('Best F1:', best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow;\">**5) SAVE THE MODELS**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for model deployment\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. - ### <span style=\"color:pink\">Save the Regression Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_regression_model, open('best_regression_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. - ### <span style=\"color:pink\">Save the Classification Model of **Method-1**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_classification_model_1, open('best_classification_model_1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. - ### <span style=\"color:pink\">Save the Classification Model of **Method-2**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_classification_model_2, open('best_classification_model_2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow;\">**6) LOAD THE MODELS**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. - ### <span style=\"color:pink\">Load the Regression Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model_load = pickle.load(open('best_regression_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. - ### <span style=\"color:pink\">Load the Classification Model-1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_1_load = pickle.load(open('best_classification_model_1.pkl', 'rb'))\n",
    "# Here rb means read binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. - ### <span style=\"color:pink\">Load the Classification Model-2</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_2_load = pickle.load(open('best_classification_model_2.pkl', 'rb'))\n",
    "# Here rb means read binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:yellow;\">**7) PREDICT THE MODELS**</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'KNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Ensure new_X is in the appropriate format (numeric values)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m new_X \u001b[38;5;241m=\u001b[39m [[best_classifier]]  \u001b[38;5;66;03m# Provide the actual numeric values\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mregression_model_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print the predictions\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:519\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    518\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:1786\u001b[0m, in \u001b[0;36mGradientBoostingRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict regression target for X.\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m \n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1786\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;66;03m# In regression we can directly return the raw value from the trees.\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'KNN'"
     ]
    }
   ],
   "source": [
    "# Ensure new_X is in the appropriate format (numeric values)\n",
    "new_X = [[best_classifier]]  # Provide the actual numeric values\n",
    "\n",
    "predictions = regression_model_load.predict(new_X)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main Assignment:**\n",
    "\n",
    "## Write the complete code to select the best Regressor and classifier for the given dataset called diamonds `(if you have a high end machine, you can use the whole dataset, else use the sample dataset provided in the link)` or you can use Tips datset for Regression task and Iris dataset for Classification task.\n",
    "\n",
    "## You have to choose all possible models with their best or possible hyperparameters and compare them with each other and select the best model for the given dataset.\n",
    "\n",
    "## Your code should be complete and explained properly. for layman, each and every step of the code should be commented properly.\n",
    "\n",
    "## You code should also save the best model in the pickle file.\n",
    "\n",
    "## You should also write the code to load the pickle file and use it for prediction. in the last snippet of the code.\n",
    "\n",
    "## Submit your assignment to the discord inbox. (Do not share the link of your notebook, just upload the notebook in the discord inbox). Do not share the notebook in public channels on our discord server.\n",
    "\n",
    "\n",
    "# **Deadline for Submission:**\n",
    "\n",
    "## `29th December before 09:30 pm Pakistan time. (No late submission will be accepted).`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time: 106.70 seconds\n"
     ]
    }
   ],
   "source": [
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total run time\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Print the total run time in seconds\n",
    "print(\"Total run time: {:.2f} seconds\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time (mm:ss): 01:46\n"
     ]
    }
   ],
   "source": [
    "# Convert the time into minutes, and seconds is stored in the variable 'total_time'\n",
    "# Convert seconds to minutes and seconds\n",
    "minutes, seconds = divmod(total_time, 60)\n",
    "# Format the time as \"mm:ss\"\n",
    "time_format = \"{:02d}:{:02d}\".format(int(minutes), int(seconds))\n",
    "# Print the formatted time\n",
    "print(\"Total run time (mm:ss): {}\".format(time_format))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
